{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ankit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Feature_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>norm_wmd</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>skew_q1vec</th>\n",
       "      <th>skew_q2vec</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217555</td>\n",
       "      <td>0.037908</td>\n",
       "      <td>3.774843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.949318</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.125323</td>\n",
       "      <td>0.137314</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>0.108845</td>\n",
       "      <td>0.344742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.368796</td>\n",
       "      <td>0.574596</td>\n",
       "      <td>15.130415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>190.766894</td>\n",
       "      <td>1.072004</td>\n",
       "      <td>0.482108</td>\n",
       "      <td>0.648993</td>\n",
       "      <td>0.027151</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>0.310524</td>\n",
       "      <td>0.033802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639209</td>\n",
       "      <td>0.215223</td>\n",
       "      <td>8.840496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.849174</td>\n",
       "      <td>0.656084</td>\n",
       "      <td>0.305829</td>\n",
       "      <td>0.332821</td>\n",
       "      <td>0.247069</td>\n",
       "      <td>0.152550</td>\n",
       "      <td>0.042899</td>\n",
       "      <td>-0.489378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  norm_wmd  \\\n",
       "0  What is the step by step guide to invest in sh...             0  0.217555   \n",
       "1  What would happen if the Indian government sto...             0  1.368796   \n",
       "2  How can Internet speed be increased by hacking...             0  0.639209   \n",
       "\n",
       "   cosine_distance  cityblock_distance  jaccard_distance  canberra_distance  \\\n",
       "0         0.037908            3.774843               1.0          75.949318   \n",
       "1         0.574596           15.130415               1.0         190.766894   \n",
       "2         0.215223            8.840496               1.0         135.849174   \n",
       "\n",
       "   euclidean_distance  minkowski_distance  braycurtis_distance  skew_q1vec  \\\n",
       "0            0.275348            0.125323             0.137314    0.008893   \n",
       "1            1.072004            0.482108             0.648993    0.027151   \n",
       "2            0.656084            0.305829             0.332821    0.247069   \n",
       "\n",
       "   skew_q2vec  kur_q1vec  kur_q2vec  \n",
       "0   -0.099771   0.108845   0.344742  \n",
       "1    0.060190   0.310524   0.033802  \n",
       "2    0.152550   0.042899  -0.489378  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('FEATURES_III.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    #REMOVING PUNTUATIONS AND NUMBERS\n",
    "    import string\n",
    "    text1 = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    text2 = re.sub('[0-9]+', \"\", text1)\n",
    "    #TOKENIZATION\n",
    "    text3 = re.split('\\W+', text2)\n",
    "    #STOPWORDS\n",
    "    text4 = [word for word in text3 if word not in stop_words]\n",
    "    #lemitization\n",
    "    lemma  = WordNetLemmatizer()\n",
    "    text5 = [lemma.lemmatize(word) for word in text4]\n",
    "    return text5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function For Basic Fetautre Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(data):\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "    data['diff_len'] = data.len_q1 - data.len_q2\n",
    "    data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "    data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning dataset\n",
    "data['question1'] = data['question1'].apply(lambda x:cleanText(x))\n",
    "data['question2'] = data['question2'].apply(lambda x:cleanText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>norm_wmd</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>skew_q1vec</th>\n",
       "      <th>skew_q2vec</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[step, step, guide, invest, share, market, india]</td>\n",
       "      <td>[step, step, guide, invest, share, market]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217555</td>\n",
       "      <td>0.037908</td>\n",
       "      <td>3.774843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.949318</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.125323</td>\n",
       "      <td>0.137314</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>0.108845</td>\n",
       "      <td>0.344742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[story, kohinoor, kohinoor, diamond]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.368796</td>\n",
       "      <td>0.574596</td>\n",
       "      <td>15.130415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>190.766894</td>\n",
       "      <td>1.072004</td>\n",
       "      <td>0.482108</td>\n",
       "      <td>0.648993</td>\n",
       "      <td>0.027151</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>0.310524</td>\n",
       "      <td>0.033802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  [step, step, guide, invest, share, market, india]   \n",
       "1               [story, kohinoor, kohinoor, diamond]   \n",
       "\n",
       "                                           question2  is_duplicate  norm_wmd  \\\n",
       "0         [step, step, guide, invest, share, market]             0  0.217555   \n",
       "1  [would, happen, indian, government, stole, koh...             0  1.368796   \n",
       "\n",
       "   cosine_distance  cityblock_distance  jaccard_distance  canberra_distance  \\\n",
       "0         0.037908            3.774843               1.0          75.949318   \n",
       "1         0.574596           15.130415               1.0         190.766894   \n",
       "\n",
       "   euclidean_distance  minkowski_distance  braycurtis_distance  skew_q1vec  \\\n",
       "0            0.275348            0.125323             0.137314    0.008893   \n",
       "1            1.072004            0.482108             0.648993    0.027151   \n",
       "\n",
       "   skew_q2vec  kur_q1vec  kur_q2vec  \n",
       "0   -0.099771   0.108845   0.344742  \n",
       "1    0.060190   0.310524   0.033802  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Features Extraction\n",
    "data = features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>norm_wmd</th>\n",
       "      <th>cosine_distance</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzz_qratio</th>\n",
       "      <th>fuzz_WRatio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>fuzz_partial_token_set_ratio</th>\n",
       "      <th>fuzz_partial_token_sort_ratio</th>\n",
       "      <th>fuzz_token_set_ratio</th>\n",
       "      <th>fuzz_token_sort_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[step, step, guide, invest, share, market, india]</td>\n",
       "      <td>[step, step, guide, invest, share, market]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217555</td>\n",
       "      <td>0.037908</td>\n",
       "      <td>3.774843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.949318</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.125323</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[story, kohinoor, kohinoor, diamond]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.368796</td>\n",
       "      <td>0.574596</td>\n",
       "      <td>15.130415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>190.766894</td>\n",
       "      <td>1.072004</td>\n",
       "      <td>0.482108</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[increase, speed, internet, connection, using,...</td>\n",
       "      <td>[internet, speed, increased, hacking, dns]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639209</td>\n",
       "      <td>0.215223</td>\n",
       "      <td>8.840496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.849174</td>\n",
       "      <td>0.656084</td>\n",
       "      <td>0.305829</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>78</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[mentally, lonely, solve]</td>\n",
       "      <td>[find, remainder, mathmath, divided, ]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.263719</td>\n",
       "      <td>0.635976</td>\n",
       "      <td>15.828719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>190.804061</td>\n",
       "      <td>1.127809</td>\n",
       "      <td>0.501902</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>48</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[one, dissolve, water, quikly, sugar, salt, me...</td>\n",
       "      <td>[fish, would, survive, salt, water]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.240908</td>\n",
       "      <td>0.332839</td>\n",
       "      <td>11.447098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.884094</td>\n",
       "      <td>0.815891</td>\n",
       "      <td>0.364076</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>86</td>\n",
       "      <td>62</td>\n",
       "      <td>100</td>\n",
       "      <td>55</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  [step, step, guide, invest, share, market, india]   \n",
       "1               [story, kohinoor, kohinoor, diamond]   \n",
       "2  [increase, speed, internet, connection, using,...   \n",
       "3                          [mentally, lonely, solve]   \n",
       "4  [one, dissolve, water, quikly, sugar, salt, me...   \n",
       "\n",
       "                                           question2  is_duplicate  norm_wmd  \\\n",
       "0         [step, step, guide, invest, share, market]             0  0.217555   \n",
       "1  [would, happen, indian, government, stole, koh...             0  1.368796   \n",
       "2         [internet, speed, increased, hacking, dns]             0  0.639209   \n",
       "3             [find, remainder, mathmath, divided, ]             0  1.263719   \n",
       "4                [fish, would, survive, salt, water]             0  1.240908   \n",
       "\n",
       "   cosine_distance  cityblock_distance  jaccard_distance  canberra_distance  \\\n",
       "0         0.037908            3.774843               1.0          75.949318   \n",
       "1         0.574596           15.130415               1.0         190.766894   \n",
       "2         0.215223            8.840496               1.0         135.849174   \n",
       "3         0.635976           15.828719               1.0         190.804061   \n",
       "4         0.332839           11.447098               1.0         154.884094   \n",
       "\n",
       "   euclidean_distance  minkowski_distance  ...  len_word_q1  len_word_q2  \\\n",
       "0            0.275348            0.125323  ...            7            6   \n",
       "1            1.072004            0.482108  ...            4            9   \n",
       "2            0.656084            0.305829  ...            6            5   \n",
       "3            1.127809            0.501902  ...            3            5   \n",
       "4            0.815891            0.364076  ...           10            5   \n",
       "\n",
       "   common_words  fuzz_qratio  fuzz_WRatio  fuzz_partial_ratio  \\\n",
       "0             5           92           95                  98   \n",
       "1             1           58           86                  91   \n",
       "2             1           64           66                  67   \n",
       "3             0           39           39                  48   \n",
       "4             1           48           86                  62   \n",
       "\n",
       "   fuzz_partial_token_set_ratio  fuzz_partial_token_sort_ratio  \\\n",
       "0                           100                             91   \n",
       "1                           100                             84   \n",
       "2                           100                             78   \n",
       "3                            33                             33   \n",
       "4                           100                             55   \n",
       "\n",
       "   fuzz_token_set_ratio  fuzz_token_sort_ratio  \n",
       "0                   100                     92  \n",
       "1                    84                     59  \n",
       "2                    70                     70  \n",
       "3                    27                     27  \n",
       "4                    51                     40  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Complete Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('completeFeatures.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
