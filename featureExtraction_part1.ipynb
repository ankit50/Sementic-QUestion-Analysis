{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rf0B6uTVUKj_"
   },
   "source": [
    "# Intalling & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "NjOYaC9IUdpW",
    "outputId": "2f34c791-7db7-4a5f-afb2-9491207cd72c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ankit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJigwsZUWfNq"
   },
   "source": [
    "# Function For Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8c96IxdWkn5"
   },
   "outputs": [],
   "source": [
    "def importDataset():\n",
    "    data = pd.read_csv('/content/drive/My Drive/majorProject/quora_duplicate_questions.tsv', sep='\\t')\n",
    "    data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYy2dfL3a89W"
   },
   "source": [
    "# WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8hOxKXZc2mO"
   },
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    \n",
    "    lemma  = WordNetLemmatizer()\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    s1 = [lemma.lemmatize(w) for w in s1 if w not in stopwords.words('english')]\n",
    "    s2 = [lemma.lemmatize(w) for w in s2 if w not in stopwords.words('english')]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_MGf7Ht2c_ng"
   },
   "source": [
    "# Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxl0DOm2dCie"
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BUJaI8xdPmy"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "JmgN5VsCdTMz",
    "outputId": "b6cf7396-a7c7-4cee-f9bc-24c074f092ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  ... is_duplicate\n",
       "0  What is the step by step guide to invest in sh...  ...            0\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...  ...            0\n",
       "2  How can I increase the speed of my internet co...  ...            0\n",
       "3  Why am I mentally very lonely? How can I solve...  ...            0\n",
       "4  Which one dissolve in water quikly sugar, salt...  ...            0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = importDataset()\n",
    "dataset.dropna(axis=0, how='any', inplace=True)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GoogleNewsVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "virwnF-PPJ7r"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/majorProject/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Normalized_WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvVZdU4KPPRz"
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)\n",
    "dataset['norm_wmd'] = dataset.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfRb7BNsWNhM"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('FEATURE_II.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHCY9ZWnWZPr"
   },
   "outputs": [],
   "source": [
    "question1_vectors = np.zeros((dataset.shape[0], 300))\n",
    "question2_vectors  = np.zeros((dataset.shape[0], 300))\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Question Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "hgJVgcdUXMfO",
    "outputId": "f1a34248-c8bc-4b21-e8bf-09cee22e0c6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [00:00, 477.70it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404287it [11:23, 591.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, q in tqdm(enumerate(dataset.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "fE7X-394aHMp",
    "outputId": "fc295926-2c7e-4088-9854-70c89e117f48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203it [00:00, 486.98it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404287it [11:35, 580.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, q in tqdm(enumerate(dataset.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "NzP3Aweuc6ta",
    "outputId": "21c7b09f-aed6-4938-8e1b-4ecd2daa6df8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:1160: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "dataset['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "dataset['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "dataset['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "dataset['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "dataset['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('features_I.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocesssing2",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
